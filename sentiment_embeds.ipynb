{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-19 09:36:55.007283: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Embedding, GlobalAveragePooling1D, Conv2D, ConvLSTM2D, ConvLSTM1D, Input, Flatten, Reshape, TextVectorization\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "from tensorflow.keras.preprocessing.text import one_hot\n",
    "from tensorflow.keras.utils import pad_sequences\n",
    "#from tensorflow.keras import ops\n",
    "import keras\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample gains:  [0, 0.1778425655976677, 1.2277227722772277, -0.6666666666666666, -0.9833333333333333]\n",
      "smaple vars:  [0, 0, 3.0500000000000007, 24.8, 30.0]\n",
      "zipped:  [[0, 0], [0.1778425655976677, 0], [1.2277227722772277, 3.0500000000000007], [-0.6666666666666666, 24.8], [-0.9833333333333333, 30.0]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Vars\"\"\"\n",
    "sample_headlines = [\"Hoo Hoo\", \"HOO\", \"WHOSE TOES\", \"HOO\", \"Hoo hoo hoo\"]\n",
    "sample_prices = [34.3, 40.4, 90, 30, 0.5]\n",
    "sample_gains = [0] + [(sample_prices[i+1]-sample_prices[i])/sample_prices[i] for i in range(0, len(sample_prices)-1)]\n",
    "var_duration = 2\n",
    "sample_vars = [0 for i in range(var_duration)] + [np.std(sample_prices[i:i+var_duration]) for i in range(0, len(sample_prices)-var_duration)]\n",
    "print(\"sample gains: \", sample_gains)\n",
    "print(\"smaple vars: \", sample_vars)\n",
    "#print(\"zipped: \", np.array(zip(np.array(sample_gains), np.array(sample_vars))))\n",
    "zipped_labels = [[sample_gains[i], sample_vars[i]] for i in range(len(sample_gains))]\n",
    "print(\"zipped: \", zipped_labels)\n",
    "\n",
    "vocab_size = 50\n",
    "max_len = 30\n",
    "embeddings_dim = 1#5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Encoder Methods\"\"\"\n",
    "def get_one_hot_encoded_batch(vocab_size, strings):\n",
    "    return [one_hot(string, vocab_size) for string in strings]\n",
    "\n",
    "def pad_input(max_len, one_hot_encoded_strings):\n",
    "    return pad_sequences(one_hot_encoded_strings, maxlen=max_len)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_6 (InputLayer)        [(None, 30)]              0         \n",
      "                                                                 \n",
      " embedding_5 (Embedding)     (None, 30, 1)             50        \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 30)                3840      \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 30)                930       \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 31        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4851 (18.95 KB)\n",
      "Trainable params: 4851 (18.95 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Encoder Architecture\"\"\"\n",
    "input = Input(shape=(max_len))\n",
    "embeddings_1 = Embedding(input_dim=vocab_size, output_dim=embeddings_dim, input_length=max_len)(input) # look at TextVecotirzation\n",
    "#flatten_1 = Flatten()(embeddings_1) \n",
    "#reshape_1 = Reshape((max_len, -1))(embeddings_1)\n",
    "lstm_1 = LSTM(units=30, return_sequences=False)(embeddings_1) # Default activation tanh - VERIFY\n",
    "dense_1 = Dense(units=30)(lstm_1)\n",
    "dense_2 = Dense(units=1)(dense_1)\n",
    "output = dense_2 # Perhaps more to come\n",
    "\n",
    "encoder = keras.Model(inputs = input, outputs = output)\n",
    "encoder.compile(optimizer=\"rmsprop\", loss=\"mse\")\n",
    "print(encoder.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Encoder Architecture 2\\ninput = Input(shape=(max_len, vocab_size))\\nembeddings_1 = Embedding(input_dim=vocab_size, output_dim=embeddings_dim, input_length=max_len)(input)\\n#flatten_1 = Flatten()(embeddings_1)\\nreshape_1 = Reshape((max_len, vocab_size))(embeddings_1)\\nlstm_1 = LSTM(units=30, return_sequences=False)(reshape_1) # Default activation tanh - VERIFY\\ndense_1 = Dense(units=30)(lstm_1)\\ndense_2 = Dense(units=2)(dense_1)\\noutput = dense_2 # Perhaps more to come\\n\\nencoder = keras.Model(inputs = input, outputs = output)\\nencoder.compile(optimizer=\"rmsprop\", loss=\"mse\")\\nprint(encoder.summary())'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Encoder Architecture 2\n",
    "input = Input(shape=(max_len, vocab_size))\n",
    "embeddings_1 = Embedding(input_dim=vocab_size, output_dim=embeddings_dim, input_length=max_len)(input)\n",
    "#flatten_1 = Flatten()(embeddings_1)\n",
    "reshape_1 = Reshape((max_len, vocab_size))(embeddings_1)\n",
    "lstm_1 = LSTM(units=30, return_sequences=False)(reshape_1) # Default activation tanh - VERIFY\n",
    "dense_1 = Dense(units=30)(lstm_1)\n",
    "dense_2 = Dense(units=2)(dense_1)\n",
    "output = dense_2 # Perhaps more to come\n",
    "\n",
    "encoder = keras.Model(inputs = input, outputs = output)\n",
    "encoder.compile(optimizer=\"rmsprop\", loss=\"mse\")\n",
    "print(encoder.summary())\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0 26 26]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0 26]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  3 12]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0 26]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0 26 26 26]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Data Processing\"\"\"\n",
    "one_hots = get_one_hot_encoded_batch(vocab_size=vocab_size, strings=sample_headlines)\n",
    "padded_one_hots = pad_input(max_len=max_len, one_hot_encoded_strings=one_hots)\n",
    "print(padded_one_hots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 4s 4s/step - loss: 152.8120\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1288c8a90>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Model Training\"\"\"\n",
    "encoder.fit(x=padded_one_hots, y=np.array(zipped_labels), epochs=10)#np.array(zip(sample_gains, sample_vars)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
