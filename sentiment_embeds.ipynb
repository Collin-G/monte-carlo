{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Embedding, GlobalAveragePooling1D, Conv2D, ConvLSTM2D, ConvLSTM1D, Input, Flatten, Reshape, TextVectorization\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "from tensorflow.keras.preprocessing.text import one_hot\n",
    "from tensorflow.keras.utils import pad_sequences\n",
    "#from tensorflow.keras import ops\n",
    "import keras\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample gains:  [0, 0.1778425655976677, 1.2277227722772277, -0.6666666666666666, -0.9833333333333333]\n",
      "smaple vars:  [0, 0, 3.0500000000000007, 24.8, 30.0]\n",
      "zipped:  [[0, 0], [0.1778425655976677, 0], [1.2277227722772277, 3.0500000000000007], [-0.6666666666666666, 24.8], [-0.9833333333333333, 30.0]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Vars\"\"\"\n",
    "sample_headlines = [\"Hoo Hoo\", \"HOO\", \"WHOSE TOES\", \"HOO\", \"Hoo hoo hoo\"]\n",
    "sample_prices = [34.3, 40.4, 90, 30, 0.5]\n",
    "sample_gains = [0] + [(sample_prices[i+1]-sample_prices[i])/sample_prices[i] for i in range(0, len(sample_prices)-1)]\n",
    "var_duration = 2\n",
    "sample_vars = [0 for i in range(var_duration)] + [np.std(sample_prices[i:i+var_duration]) for i in range(0, len(sample_prices)-var_duration)]\n",
    "print(\"sample gains: \", sample_gains)\n",
    "print(\"smaple vars: \", sample_vars)\n",
    "#print(\"zipped: \", np.array(zip(np.array(sample_gains), np.array(sample_vars))))\n",
    "zipped_labels = [[sample_gains[i], sample_vars[i]] for i in range(len(sample_gains))]\n",
    "print(\"zipped: \", zipped_labels)\n",
    "\n",
    "vocab_size = 50\n",
    "max_len = 30\n",
    "embeddings_dim = 1#5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Encoder Methods\"\"\"\n",
    "def get_one_hot_encoded_batch(vocab_size, strings):\n",
    "    return [one_hot(string, vocab_size) for string in strings]\n",
    "\n",
    "def pad_input(max_len, one_hot_encoded_strings):\n",
    "    return pad_sequences(one_hot_encoded_strings, maxlen=max_len)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_10\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_14 (InputLayer)       [(None, 30, 50)]          0         \n",
      "                                                                 \n",
      " embedding_12 (Embedding)    (None, 30, 50, 1)         50        \n",
      "                                                                 \n",
      " reshape_12 (Reshape)        (None, 30, 50)            0         \n",
      "                                                                 \n",
      " lstm_10 (LSTM)              (None, 30)                9720      \n",
      "                                                                 \n",
      " dense_20 (Dense)            (None, 30)                930       \n",
      "                                                                 \n",
      " dense_21 (Dense)            (None, 1)                 31        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 10731 (41.92 KB)\n",
      "Trainable params: 10731 (41.92 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Encoder Architecture\"\"\"\n",
    "input = Input(shape=(max_len, vocab_size))\n",
    "embeddings_1 = Embedding(input_dim=vocab_size, output_dim=embeddings_dim, input_length=max_len)(input)\n",
    "#flatten_1 = Flatten()(embeddings_1)\n",
    "reshape_1 = Reshape((max_len, vocab_size))(embeddings_1)\n",
    "lstm_1 = LSTM(units=30, return_sequences=False)(reshape_1) # Default activation tanh - VERIFY\n",
    "dense_1 = Dense(units=30)(lstm_1)\n",
    "dense_2 = Dense(units=1)(dense_1)\n",
    "output = dense_2 # Perhaps more to come\n",
    "\n",
    "encoder = keras.Model(inputs = input, outputs = output)\n",
    "encoder.compile(optimizer=\"rmsprop\", loss=\"mse\")\n",
    "print(encoder.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Encoder Architecture 2\\ninput = Input(shape=(max_len, vocab_size))\\nembeddings_1 = Embedding(input_dim=vocab_size, output_dim=embeddings_dim, input_length=max_len)(input)\\n#flatten_1 = Flatten()(embeddings_1)\\nreshape_1 = Reshape((max_len, vocab_size))(embeddings_1)\\nlstm_1 = LSTM(units=30, return_sequences=False)(reshape_1) # Default activation tanh - VERIFY\\ndense_1 = Dense(units=30)(lstm_1)\\ndense_2 = Dense(units=2)(dense_1)\\noutput = dense_2 # Perhaps more to come\\n\\nencoder = keras.Model(inputs = input, outputs = output)\\nencoder.compile(optimizer=\"rmsprop\", loss=\"mse\")\\nprint(encoder.summary())'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Encoder Architecture 2\n",
    "input = Input(shape=(max_len, vocab_size))\n",
    "embeddings_1 = Embedding(input_dim=vocab_size, output_dim=embeddings_dim, input_length=max_len)(input)\n",
    "#flatten_1 = Flatten()(embeddings_1)\n",
    "reshape_1 = Reshape((max_len, vocab_size))(embeddings_1)\n",
    "lstm_1 = LSTM(units=30, return_sequences=False)(reshape_1) # Default activation tanh - VERIFY\n",
    "dense_1 = Dense(units=30)(lstm_1)\n",
    "dense_2 = Dense(units=2)(dense_1)\n",
    "output = dense_2 # Perhaps more to come\n",
    "\n",
    "encoder = keras.Model(inputs = input, outputs = output)\n",
    "encoder.compile(optimizer=\"rmsprop\", loss=\"mse\")\n",
    "print(encoder.summary())\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  3  3]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  3]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0 17 10]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  3]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  3  3  3]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Data Processing\"\"\"\n",
    "one_hots = get_one_hot_encoded_batch(vocab_size=vocab_size, strings=sample_headlines)\n",
    "padded_one_hots = pad_input(max_len=max_len, one_hot_encoded_strings=one_hots)\n",
    "print(padded_one_hots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/opt/homebrew/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"/opt/homebrew/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1384, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/opt/homebrew/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1373, in run_step  **\n        outputs = model.train_step(data)\n    File \"/opt/homebrew/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1150, in train_step\n        y_pred = self(x, training=True)\n    File \"/opt/homebrew/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/opt/homebrew/lib/python3.11/site-packages/keras/src/layers/reshaping/reshape.py\", line 118, in _fix_unknown_dimension\n        raise ValueError(msg)\n\n    ValueError: Exception encountered when calling layer 'reshape_10' (type Reshape).\n    \n    total size of new array must be unchanged, input_shape = [30, 1], output_shape = [30, 50]\n    \n    Call arguments received by layer 'reshape_10' (type Reshape):\n      â€¢ inputs=tf.Tensor(shape=(None, 30, 1), dtype=float32)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"Model Training\"\"\"\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mencoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadded_one_hots\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mzipped_labels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;66;03m#np.array(zip(sample_gains, sample_vars)))\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/var/folders/2t/dh69lmc920l_h8vbm87mpzcw0000gn/T/__autograph_generated_filekvnlbcvo.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/opt/homebrew/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"/opt/homebrew/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1384, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/opt/homebrew/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1373, in run_step  **\n        outputs = model.train_step(data)\n    File \"/opt/homebrew/lib/python3.11/site-packages/keras/src/engine/training.py\", line 1150, in train_step\n        y_pred = self(x, training=True)\n    File \"/opt/homebrew/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/opt/homebrew/lib/python3.11/site-packages/keras/src/layers/reshaping/reshape.py\", line 118, in _fix_unknown_dimension\n        raise ValueError(msg)\n\n    ValueError: Exception encountered when calling layer 'reshape_10' (type Reshape).\n    \n    total size of new array must be unchanged, input_shape = [30, 1], output_shape = [30, 50]\n    \n    Call arguments received by layer 'reshape_10' (type Reshape):\n      â€¢ inputs=tf.Tensor(shape=(None, 30, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Model Training\"\"\"\n",
    "encoder.fit(x=padded_one_hots, y=np.array(zipped_labels))#np.array(zip(sample_gains, sample_vars)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
