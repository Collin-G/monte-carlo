{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Embedding, GlobalAveragePooling1D, Conv2D, ConvLSTM2D, ConvLSTM1D, Input, Flatten, Reshape, TextVectorization\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "from tensorflow.keras.preprocessing.text import one_hot\n",
    "from tensorflow.keras.utils import pad_sequences\n",
    "#from tensorflow.keras import ops\n",
    "import keras\n",
    "import numpy as np\n",
    "\n",
    "tf.config.run_functions_eagerly(True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample gains:  [0, 0.1778425655976677, 1.2277227722772277, -0.6666666666666666, -0.9833333333333333]\n",
      "smaple vars:  [0, 0, 3.0500000000000007, 24.8, 30.0]\n",
      "zipped:  [[0, 0], [0.1778425655976677, 0], [1.2277227722772277, 3.0500000000000007], [-0.6666666666666666, 24.8], [-0.9833333333333333, 30.0]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Vars\"\"\"\n",
    "sample_headlines = [\"Hoo Hoo\", \"HOO\", \"WHOSE TOES\", \"HOO\", \"Hoo hoo hoo\"]\n",
    "sample_prices = [34.3, 40.4, 90, 30, 0.5]\n",
    "sample_gains = [0] + [(sample_prices[i+1]-sample_prices[i])/sample_prices[i] for i in range(0, len(sample_prices)-1)]\n",
    "var_duration = 2\n",
    "sample_vars = [0 for i in range(var_duration)] + [np.std(sample_prices[i:i+var_duration]) for i in range(0, len(sample_prices)-var_duration)]\n",
    "print(\"sample gains: \", sample_gains)\n",
    "print(\"smaple vars: \", sample_vars)\n",
    "#print(\"zipped: \", np.array(zip(np.array(sample_gains), np.array(sample_vars))))\n",
    "zipped_labels = [[sample_gains[i], sample_vars[i]] for i in range(len(sample_gains))]\n",
    "print(\"zipped: \", zipped_labels)\n",
    "\n",
    "vocab_size = 50\n",
    "max_len = 30\n",
    "embeddings_dim = 1#5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Encoder Methods\"\"\"\n",
    "def get_one_hot_encoded_batch(vocab_size, strings):\n",
    "    return [one_hot(string, vocab_size) for string in strings]\n",
    "\n",
    "def pad_input(max_len, one_hot_encoded_strings):\n",
    "    return pad_sequences(one_hot_encoded_strings, maxlen=max_len)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_14\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_19 (InputLayer)       [(None, 30)]              0         \n",
      "                                                                 \n",
      " embedding_17 (Embedding)    (None, 30, 1)             50        \n",
      "                                                                 \n",
      " lstm_14 (LSTM)              (None, 30)                3840      \n",
      "                                                                 \n",
      " dense_28 (Dense)            (None, 30)                930       \n",
      "                                                                 \n",
      " dense_29 (Dense)            (None, 1)                 31        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4851 (18.95 KB)\n",
      "Trainable params: 4851 (18.95 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Encoder Architecture\"\"\"\n",
    "input = Input(shape=(max_len))\n",
    "embeddings_1 = Embedding(input_dim=vocab_size, output_dim=embeddings_dim, input_length=max_len)(input) # look at TextVecotirzation\n",
    "#flatten_1 = Flatten()(embeddings_1) \n",
    "#reshape_1 = Reshape((max_len, -1))(embeddings_1)\n",
    "lstm_1 = LSTM(units=30, return_sequences=False)(embeddings_1) # Default activation tanh - VERIFY\n",
    "dense_1 = Dense(units=30)(lstm_1)\n",
    "dense_2 = Dense(units=1)(dense_1)\n",
    "output = dense_2 # Perhaps more to come\n",
    "\n",
    "encoder = keras.Model(inputs = input, outputs = output)\n",
    "encoder.compile(optimizer=\"rmsprop\", loss=\"mse\")\n",
    "print(encoder.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Encoder Architecture 2\\ninput = Input(shape=(max_len, vocab_size))\\nembeddings_1 = Embedding(input_dim=vocab_size, output_dim=embeddings_dim, input_length=max_len)(input)\\n#flatten_1 = Flatten()(embeddings_1)\\nreshape_1 = Reshape((max_len, vocab_size))(embeddings_1)\\nlstm_1 = LSTM(units=30, return_sequences=False)(reshape_1) # Default activation tanh - VERIFY\\ndense_1 = Dense(units=30)(lstm_1)\\ndense_2 = Dense(units=2)(dense_1)\\noutput = dense_2 # Perhaps more to come\\n\\nencoder = keras.Model(inputs = input, outputs = output)\\nencoder.compile(optimizer=\"rmsprop\", loss=\"mse\")\\nprint(encoder.summary())'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Encoder Architecture 2\n",
    "input = Input(shape=(max_len, vocab_size))\n",
    "embeddings_1 = Embedding(input_dim=vocab_size, output_dim=embeddings_dim, input_length=max_len)(input)\n",
    "#flatten_1 = Flatten()(embeddings_1)\n",
    "reshape_1 = Reshape((max_len, vocab_size))(embeddings_1)\n",
    "lstm_1 = LSTM(units=30, return_sequences=False)(reshape_1) # Default activation tanh - VERIFY\n",
    "dense_1 = Dense(units=30)(lstm_1)\n",
    "dense_2 = Dense(units=2)(dense_1)\n",
    "output = dense_2 # Perhaps more to come\n",
    "\n",
    "encoder = keras.Model(inputs = input, outputs = output)\n",
    "encoder.compile(optimizer=\"rmsprop\", loss=\"mse\")\n",
    "print(encoder.summary())\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  3  3]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  3]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0 17 10]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  3]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  3  3  3]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Data Processing\"\"\"\n",
    "one_hots = get_one_hot_encoded_batch(vocab_size=vocab_size, strings=sample_headlines)\n",
    "padded_one_hots = pad_input(max_len=max_len, one_hot_encoded_strings=one_hots)\n",
    "print(padded_one_hots)\n",
    "dataset = tf.data.Dataset.from_tensor_slices(sample_headlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 152.7209\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 152.0657\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 151.5071\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 150.9084\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/tensorflow/python/data/ops/structured_function.py:258: UserWarning: Even though the `tf.config.experimental_run_functions_eagerly` option is set, this option does not apply to tf.data functions. To force eager execution of tf.data functions, please use `tf.data.experimental.enable_debug_mode()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 31ms/step - loss: 150.2088\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 149.3429\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 148.2097\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 146.6267\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 144.2387\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 140.4003\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x2966e6f90>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Model Training\"\"\"\n",
    "encoder.fit(x=padded_one_hots, y=np.array(zipped_labels), epochs=10)#np.array(zip(sample_gains, sample_vars)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_16\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_21 (InputLayer)       [(None, 1)]               0         \n",
      "                                                                 \n",
      " text_vectorization_4 (Text  (None, 30)                0         \n",
      " Vectorization)                                                  \n",
      "                                                                 \n",
      " embedding_19 (Embedding)    (None, 30, 1)             50        \n",
      "                                                                 \n",
      " lstm_16 (LSTM)              (None, 30)                3840      \n",
      "                                                                 \n",
      " dense_32 (Dense)            (None, 30)                930       \n",
      "                                                                 \n",
      " dense_33 (Dense)            (None, 2)                 62        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4882 (19.07 KB)\n",
      "Trainable params: 4882 (19.07 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Encoder Architecture 3\"\"\"\n",
    "input = Input(shape=(1, ), dtype=tf.string)\n",
    "\n",
    "tv_layer = TextVectorization(max_tokens=1000, output_mode='int', output_sequence_length=max_len)\n",
    "tv_layer.adapt(dataset.batch(64))\n",
    "\n",
    "tv_layer_1 = tv_layer(input)\n",
    "embeddings_1 = Embedding(input_dim=vocab_size, output_dim=embeddings_dim, input_length=max_len)(tv_layer_1)\n",
    "#flatten_1 = Flatten()(embeddings_1)\n",
    "#reshape_1 = Reshape((max_len, vocab_size))(embeddings_1)\n",
    "lstm_1 = LSTM(units=30, return_sequences=False)(embeddings_1) # Default activation tanh - VERIFY\n",
    "dense_1 = Dense(units=30)(lstm_1)\n",
    "dense_2 = Dense(units=2)(dense_1)\n",
    "output = dense_2 # Perhaps more to come\n",
    "\n",
    "encoder = keras.Model(inputs = input, outputs = output)\n",
    "encoder.compile(optimizer=\"rmsprop\", loss=\"mse\")\n",
    "print(encoder.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 152.6338\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 152.0018\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 151.4263\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 150.7928\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 150.0399\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 149.0818\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 147.7851\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 145.9216\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 143.0585\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 138.3467\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x2975bead0>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Model Training\"\"\"\n",
    "encoder.fit(x=np.array(sample_headlines), y=np.array(zipped_labels), epochs=10)#np.array(zip(sample_gains, sample_vars)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_17\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_22 (InputLayer)       [(None, 1)]               0         \n",
      "                                                                 \n",
      " text_vectorization_5 (Text  (None, 30)                0         \n",
      " Vectorization)                                                  \n",
      "                                                                 \n",
      " embedding_20 (Embedding)    (None, 30, 1)             50        \n",
      "                                                                 \n",
      " lstm_17 (LSTM)              (None, 30)                3840      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3890 (15.20 KB)\n",
      "Trainable params: 3890 (15.20 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Encoder Architecture 4\"\"\"\n",
    "input = Input(shape=(1, ), dtype=tf.string)\n",
    "\n",
    "tv_layer = TextVectorization(max_tokens=1000, output_mode='int', output_sequence_length=max_len)\n",
    "tv_layer.adapt(dataset.batch(64))\n",
    "\n",
    "tv_layer_1 = tv_layer(input)\n",
    "embeddings_1 = Embedding(input_dim=vocab_size, output_dim=embeddings_dim, input_length=max_len)(tv_layer_1)\n",
    "#flatten_1 = Flatten()(embeddings_1)\n",
    "#reshape_1 = Reshape((max_len, vocab_size))(embeddings_1)\n",
    "lstm_1 = LSTM(units=30, return_sequences=False)(embeddings_1) # Default activation tanh - VERIFY\n",
    "#dense_1 = Dense(units=30)(lstm_1)\n",
    "output = lstm_1#dense_1\n",
    "\n",
    "\n",
    "encoder = keras.Model(inputs = input, outputs = output)\n",
    "encoder.compile(optimizer=\"rmsprop\", loss=\"mse\")\n",
    "print(encoder.summary())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
