{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Embedding, GlobalAveragePooling1D, Conv2D, ConvLSTM2D, ConvLSTM1D, Input, Flatten, Reshape\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "from tensorflow.keras.preprocessing.text import one_hot\n",
    "from tensorflow.keras.utils import pad_sequences\n",
    "#from tensorflow.keras import ops\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Vars\"\"\"\n",
    "sample_headlines = [\"Hoo Hoo\", \"HOO\", \"WHOSE TOES\", \"HOO\", \"Hoo hoo hoo\"]\n",
    "sample_prices = [34.3, 40.4, 90, 30, 0.5]\n",
    "sample_gains = [0] + [(sample_prices[i+1]-sample_prices[i])/sample_prices[i] for i in range(0, len(sample_prices)-1)]\n",
    "#print(sample_gains)\n",
    "vocab_size = 50\n",
    "max_len = 30\n",
    "embeddings_dim = 1#5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Encoder Methods\"\"\"\n",
    "def get_one_hot_encoded_batch(vocab_size, strings):\n",
    "    return [one_hot(string, vocab_size) for string in strings]\n",
    "\n",
    "def pad_input(max_len, one_hot_encoded_strings):\n",
    "    return pad_sequences(one_hot_encoded_strings, maxlen=max_len)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_6 (InputLayer)        [(None, 30, 50)]          0         \n",
      "                                                                 \n",
      " embedding_5 (Embedding)     (None, 30, 50, 1)         50        \n",
      "                                                                 \n",
      " reshape_1 (Reshape)         (None, 30, 50)            0         \n",
      "                                                                 \n",
      " lstm_4 (LSTM)               (None, 30)                9720      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 30)                930       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 31        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 10731 (41.92 KB)\n",
      "Trainable params: 10731 (41.92 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Encoder Architecture\"\"\"\n",
    "input = Input(shape=(max_len, vocab_size))\n",
    "embeddings_1 = Embedding(input_dim=vocab_size, output_dim=embeddings_dim, input_length=max_len)(input)\n",
    "#flatten_1 = Flatten()(embeddings_1)\n",
    "reshape_1 = Reshape((max_len, vocab_size))(embeddings_1)\n",
    "lstm_1 = LSTM(units=30, return_sequences=False)(reshape_1) # Default activation tanh - VERIFY\n",
    "dense_1 = Dense(units=30)(lstm_1)\n",
    "dense_2 = Dense(units=1)(dense_1)\n",
    "output = dense_2 # Perhaps more to come\n",
    "\n",
    "encoder = keras.Model(inputs = input, outputs = output)\n",
    "print(encoder.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Encoder Architecture 2\"\"\"\n",
    "input = Input(shape=(max_len, vocab_size))\n",
    "embeddings_1 = Embedding(input_dim=vocab_size, output_dim=embeddings_dim, input_length=max_len)(input)\n",
    "#flatten_1 = Flatten()(embeddings_1)\n",
    "reshape_1 = Reshape((max_len, vocab_size))(embeddings_1)\n",
    "lstm_1 = LSTM(units=30, return_sequences=False)(reshape_1) # Default activation tanh - VERIFY\n",
    "dense_1 = Dense(units=30)(lstm_1)\n",
    "dense_2 = Dense(units=1)(dense_1)\n",
    "output = dense_2 # Perhaps more to come\n",
    "\n",
    "encoder = keras.Model(inputs = input, outputs = output)\n",
    "print(encoder.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Data Processing\"\"\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
